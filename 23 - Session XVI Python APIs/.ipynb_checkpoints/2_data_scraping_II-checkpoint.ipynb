{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data hunting and gathering (part 2)\n",
    "\n",
    "<img style = \"border-radius:20px;\" src = \"http://unadocenade.com/wp-content/uploads/2012/09/cavalls-de-valltorta.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 2: Creating Our Own Web API - Scraping\n",
    "\n",
    "In this session, we dive into the world of web scraping, focusing on advanced techniques and tools to extract data from dynamic web pages.\n",
    "\n",
    "## Learning Objectives\n",
    "- **Understanding HTML and CSS**: Grasp the basics of HTML structure and CSS styling to navigate web pages effectively.\n",
    "- **XPath Selectors**: Learn how to use XPath selectors to pinpoint and extract specific content from web pages.\n",
    "- **Scraping Dynamic Content with Selenium**: Understand how to scrape dynamically generated content, which standard scraping tools can't always handle.\n",
    "\n",
    "## Additional Python Libraries\n",
    "You will need to install these Python libraries for scraping tasks:\n",
    "\n",
    "- **lxml**: A powerful library for processing XML and HTML in Python.\n",
    "  - Install via pip: `pip install lxml`\n",
    "- **selenium**: An automated web browser tool for testing web applications, also useful for complex scraping tasks.\n",
    "  - Install via pip: `pip install selenium`\n",
    "\n",
    "### Note\n",
    "Ensure that all software and libraries are installed before the session. This will enable you to actively participate in the exercises and follow along with the scraping examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. \"Making Your Own API\": Web Scraping\n",
    "\n",
    "### Understanding Web Scraping\n",
    "Web scraping becomes essential when data is available on the web but isn't accessible through an API, or the existing API lacks certain functionalities or has restrictive terms of service. In such scenarios, **Web Scraping** is the technique that enables automated extraction of this data, replicating the access a human would have visually.\n",
    "\n",
    "### Why Web Scraping?\n",
    "- **Data Accessibility**: Sometimes, the only way to access certain data is directly from the web pages where it is displayed.\n",
    "- **Flexibility**: Web scraping allows you to tailor data extraction to specific needs, bypassing limitations of existing APIs.\n",
    "\n",
    "### Preparing for Web Scraping: Understanding Web Page Structure\n",
    "Before delving into scraping, it's crucial to have a basic understanding of web page structure and how data is stored and presented. This session covers:\n",
    "\n",
    "#### Basic HTML and CSS Static Pages\n",
    "- **HTML (HyperText Markup Language)**: The standard markup language used to create web pages. Understanding HTML is key to identifying the data you want to scrape.\n",
    "- **CSS (Cascading Style Sheets)**: Used for describing the presentation of a document written in HTML. Knowing CSS helps in pinpointing specific elements on a page.\n",
    "\n",
    "#### Dynamic HTML\n",
    "- **Basic JavaScript Example Using JQuery**: Websites often use JavaScript to load data dynamically. Understanding how this works is crucial for scraping data from such dynamic pages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Basic HTML + CSS 101\n",
    "\n",
    "#### Understanding the Foundation of Web Pages\n",
    "\n",
    "The most fundamental web pages are constructed using HTML and CSS. These technologies serve two primary purposes: **HTML (Hypertext Markup Language)** structures and stores the content, making it the primary target for web scraping, while **CSS (Cascading Style Sheets)** formats and styles the content, highlighting visual elements like fonts, colors, borders, and layout.\n",
    "\n",
    "#### HTML: The Structure of the Web\n",
    "HTML is a markup language typically rendered by web browsers. It uses 'tags' to define elements on a web page. A typical tag format includes a tag name, attributes (if any), and the content between opening and closing tags.\n",
    "\n",
    "#### Key Components of an HTML File\n",
    "\n",
    "- **DOCTYPE Declaration**: \n",
    "  - Begins with `<!DOCTYPE html>`, indicating the use of HTML5.\n",
    "  - Earlier HTML versions had different DOCTYPEs.\n",
    "\n",
    "- **HTML Tag**: \n",
    "  - The `html` tag (and its closing `/html` tag) encloses the entire web page content.\n",
    "\n",
    "- **Head and Body**: \n",
    "  - The `head` section often includes the `title` tag, defining the webpage's name, links to CSS stylesheets, and JavaScript files for dynamic behavior.\n",
    "  - The `body` contains the visible webpage content.\n",
    "\n",
    "- **Common HTML Elements**:\n",
    "  - **Headings and Paragraphs**: Use `h#` (where # is a number) for headings and `p` for paragraphs.\n",
    "  - **Hyperlinks**: Defined with the `href` attribute in `a` (anchor) tags.\n",
    "  - **Images**: Embedded using `img` tags with the `src` attribute. Note: `img` is self-closing.\n",
    "\n",
    "#### Exercise: Build a Basic HTML Web Page\n",
    "\n",
    "Let's put your HTML knowledge into practice:\n",
    "\n",
    "- Create a file named 'example.html' in your favorite text editor.\n",
    "- Build a basic HTML web page containing elements like `title`, `h1`, `p`, `img`, and `a` tags. Remember that nearly all tags need to be closed with a `/tag`.\n",
    "\n",
    "This exercise aims to familiarize you with the basic structure of HTML and how various elements come together to form a web page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are lazy go to the files folder and double-click on \"example.html\". You can check the html code executing the following line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<!-- Start of the HTML head section -->\n",
       "<head>\n",
       "    <!-- Title of the webpage -->\n",
       "    <title>\n",
       "        Basic knowledge for web scraping.\n",
       "    </title>\t\n",
       "</head>\n",
       "<!-- Start of the HTML body section -->\n",
       "<body>\n",
       "    <!-- Header 1 indicating the subject of the content -->\n",
       "    <h1>About HTML\n",
       "    </h1>\n",
       "    <!-- Paragraph explaining what HTML is and providing a link for further information -->\n",
       "    <p>Html (Hypertext markdown language) is the basic language to provide contents in the web. It is a tagged language. You can check more about it in <a href=\"http://www.w3.org/community/webed/wiki/HTML\">World Wide Web Consortium.</a></p>\n",
       "    \n",
       "    <!-- Paragraph indicating that one of the following images is clickable -->\n",
       "    <p> One of the following rubberduckies is clickable\n",
       "    </p>\n",
       "    <!-- Image of a rubber ducky; this one is not clickable -->\n",
       "    <p>\n",
       "        <img src = \"files/rubberduck.jpg\"/>\n",
       "    \n",
       "        <!-- Clickable image (hyperlinked) of a rubber ducky -->\n",
       "        <a href=\"http://www.pinterest.com/misscannabliss/rubber-duck-mania/\"><img src = \"files/rubberduck.jpg\"/></a>\n",
       "    </p>\n",
       "</body>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "\n",
    "<!-- Start of the HTML head section -->\n",
    "<head>\n",
    "    <!-- Title of the webpage -->\n",
    "    <title>\n",
    "        Basic knowledge for web scraping.\n",
    "    </title>\t\n",
    "</head>\n",
    "<!-- Start of the HTML body section -->\n",
    "<body>\n",
    "    <!-- Header 1 indicating the subject of the content -->\n",
    "    <h1>About HTML\n",
    "    </h1>\n",
    "    <!-- Paragraph explaining what HTML is and providing a link for further information -->\n",
    "    <p>Html (Hypertext markdown language) is the basic language to provide contents in the web. It is a tagged language. You can check more about it in <a href=\"http://www.w3.org/community/webed/wiki/HTML\">World Wide Web Consortium.</a></p>\n",
    "    \n",
    "    <!-- Paragraph indicating that one of the following images is clickable -->\n",
    "    <p> One of the following rubberduckies is clickable\n",
    "    </p>\n",
    "    <!-- Image of a rubber ducky; this one is not clickable -->\n",
    "    <p>\n",
    "        <img src = \"files/rubberduck.jpg\"/>\n",
    "    \n",
    "        <!-- Clickable image (hyperlinked) of a rubber ducky -->\n",
    "        <a href=\"http://www.pinterest.com/misscannabliss/rubber-duck-mania/\"><img src = \"files/rubberduck.jpg\"/></a>\n",
    "    </p>\n",
    "</body>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Old Style vs. Current HTML Static Pages\n",
    "\n",
    "#### Old Style HTML Static Pages\n",
    "Old style HTML pages often relied on tables and lists for structuring content. \n",
    "\n",
    "- **Lists**:\n",
    "  - **Ordered Lists (`ol`)**: Used for creating lists where order matters, with each item represented by `li` (list item).\n",
    "  - **Unordered Lists (`ul`)**: Used for lists where order is not important, again using `li` for each item.\n",
    "\n",
    "- **Tables**:\n",
    "  - The `table` tag is used to create a table.\n",
    "  - Each table row is marked with a `tr` tag.\n",
    "  - Table columns are defined by `td` (table data) elements within each row.\n",
    "  - Tables may include a header (`thead`) and a body (`tbody`).\n",
    "  - `th` elements are used similarly to `td` but for headers.\n",
    "  - To span a cell across multiple columns, use `colspan` with the number of cells to cover.\n",
    "\n",
    "#### Current HTML Static Pages\n",
    "Modern HTML pages focus more on using containers and CSS for layout and styling.\n",
    "\n",
    "- **Divisions (`div`)**: \n",
    "  - The `div` tag signifies a division and is used to define a block of content. It is a versatile container used in modern web design.\n",
    "\n",
    "- **Spans (`span`)**: \n",
    "  - The `span` tag is used to highlight or style a specific part of a block of content. It's an inline container and is often used for small-scale modifications to text or other elements.\n",
    "\n",
    "Both old and current styles of HTML have their uses, but modern practices favor the use of `div` and `span` along with CSS for more flexible and responsive design.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding CSS for Web Scraping\n",
    "\n",
    "CSS, which stands for Cascading Style Sheets, is a stylesheet language used to describe the presentation and formatting of HTML documents. In web scraping, understanding CSS is crucial for effectively navigating and extracting data from web pages.\n",
    "\n",
    "#### What is CSS?\n",
    "- **CSS** is a language designed to style the content of HTML files. By using CSS, web developers define how various HTML elements should appear on a webpage.\n",
    "- The term **\"cascading\"** refers to the priority given to specific style rules over more generic ones. This hierarchy is a fundamental aspect of CSS.\n",
    "\n",
    "#### The Role of CSS in Web Scraping\n",
    "- **Separation of Concerns**: CSS allows for a clear separation between the structure of HTML (content) and the style of the webpage (appearance). This separation makes webpages easier to design and maintain, and also easier to scrape.\n",
    "- **Selectors and Properties**:\n",
    "  - **Selectors** are patterns used to select the element(s) you want to style, or in the case of scraping, the elements you want to extract.\n",
    "  - **Properties** are the aspects of the elements you want to style, such as color, font, width, height, and more.\n",
    "- **Cascading Order**:\n",
    "  - Styles are applied in order of specificity, with more specific selectors overriding more general ones. Inline styles (directly within an HTML element) have the highest specificity.\n",
    "\n",
    "#### Example in Web Scraping\n",
    "Consider a webpage with the following HTML and CSS:\n",
    "\n",
    "```html\n",
    "<!-- HTML Example -->\n",
    "<div class=\"product-description\">\n",
    "    <p>Awesome product</p>\n",
    "</div>\n",
    "```\n",
    "\n",
    "Now, css example:\n",
    "\n",
    "```css\n",
    "/* CSS Example */\n",
    ".product-description p {\n",
    "    color: blue;\n",
    "}\n",
    "```\n",
    "\n",
    "In this example, the CSS targets a `p` element within a `div` of the class `product-description` and changes its text color to blue. Understanding how this CSS rule applies helps in scraping data accurately.\n",
    "\n",
    "### Conclusion\n",
    "For web scraping, CSS is not just about understanding webpage aesthetics; it's about comprehensively understanding the webpage's structure. This understanding is crucial for effective data extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Example: Web Scraping Using CSS Selectors\n",
    "\n",
    "This example demonstrates how to scrape a website (Python's official blog) and extract specific content using CSS selectors in Python. We use the `requests` and `BeautifulSoup` libraries to accomplish this.\n",
    "\n",
    "#### Script Explanation\n",
    "\n",
    "1. **Import Libraries**:\n",
    "   - `requests` for sending HTTP requests.\n",
    "   - `BeautifulSoup` from `bs4` for parsing HTML content.\n",
    "\n",
    "   ```python\n",
    "   import requests\n",
    "   from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install requests beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the website we want to scrape\n",
    "# In this case, we are targeting the Python.org blog page\n",
    "url = \"https://www.python.org/blogs/\"\n",
    "\n",
    "# Send a GET request to the specified URL\n",
    "# This request fetches the HTML content of the webpage\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content of the page\n",
    "# 'BeautifulSoup' is a Python library for parsing HTML documents\n",
    "# It creates a parse tree from page source code that can be used to extract data easily\n",
    "soup = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is soup if you print it:\n",
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a CSS selector to extract specific elements\n",
    "# Here, we select all 'h2' elements (commonly used for titles/headings in HTML)\n",
    "# 'select' is a method that finds all instances of a tag with the specified CSS path\n",
    "titles = soup.select('h2')\n",
    "\n",
    "# Iterate through the extracted titles and print them\n",
    "# 'get_text()' extracts the text part of the HTML element, and 'strip()' removes leading/trailing whitespaces\n",
    "for title in titles:\n",
    "    print(title.get_text().strip())\n",
    "\n",
    "# This script prints out all the text content of 'h2' tags found on the Python blog page\n",
    "# It provides an example of how to extract and print specific parts of a webpage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Scraping Exercise: Extracting News Headlines from BBC Technology\n",
    "\n",
    "#### Objective\n",
    "Write a Python script to scrape headlines from BBC's Technology news section and categorize them based on keywords.\n",
    "\n",
    "#### Task Details\n",
    "\n",
    "1. **Website to Scrape**:\n",
    "   - Target the BBC's 'Technology' section: [BBC Technology News](https://www.bbc.co.uk/news/technology).\n",
    "\n",
    "2. **Scraping Requirement**:\n",
    "   - Scrape the main headlines from the page, typically found in `h3` tags or a specific class.\n",
    "\n",
    "3. **Categorization**:\n",
    "   - Categorize the headlines based on predefined keywords like 'Apple', 'Microsoft', 'Google', etc.\n",
    "   - Count the number of headlines that fall into each category.\n",
    "\n",
    "4. **Output**:\n",
    "   - Print each headline along with its respective category.\n",
    "   - Summarize with the count of headlines in each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the BBC technology news section\n",
    "url = \"https://www.bbc.co.uk/news/technology\"\n",
    "\n",
    "# Send a GET request and parse the HTML content\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Define categories and associated keywords\n",
    "categories = {\n",
    "    'Apple': ['Apple', 'iPhone', 'iPad'],\n",
    "    'Microsoft': ['Microsoft', 'Windows', 'Bill Gates'],\n",
    "    'Google': ['Google', 'Android', 'Alphabet']\n",
    "    # Add more categories as needed\n",
    "}\n",
    "\n",
    "# Function to determine the category of a headline\n",
    "def categorize_headline(headline):\n",
    "    # Logic to determine the category based on keywords\n",
    "    # Return the category name if a keyword is found, else return 'Other'\n",
    "    pass\n",
    "\n",
    "# Scrape and process the headlines\n",
    "# Look for 'h3' tags or other relevant tags\n",
    "# Use the categorize_headline function to categorize each headline\n",
    "# Print each headline and its category\n",
    "\n",
    "# Print the count of headlines in each category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Selecting Elements with XPath\n",
    "\n",
    "XPath, or XML Path Language, is a versatile and robust tool for navigating and selecting elements within HTML documents. While Beautiful Soup and requests are commonly used libraries for web scraping, XPath offers a unique and powerful approach to extracting data from web pages.\n",
    "\n",
    "### What is XPath?\n",
    "\n",
    "XPath was originally designed for navigating XML documents, but it is equally applicable to HTML, which shares a structural similarity with XML. XPath allows you to specify the precise location of elements or data within an HTML document using a concise and expressive syntax.\n",
    "\n",
    "### Key Differentiators:\n",
    "\n",
    "Here are some key differentiators that set XPath apart from other web scraping approaches:\n",
    "\n",
    "1. **Granular Selection**: XPath provides granular control over element selection. Unlike Beautiful Soup, which often requires multiple iterations and filtering, XPath allows you to pinpoint elements directly based on their attributes, tags, or positions within the document.\n",
    "\n",
    "2. **Hierarchical Navigation**: XPath excels at navigating the hierarchical structure of HTML documents. It enables you to traverse the document tree, moving up, down, or across branches with ease.\n",
    "\n",
    "3. **Precise Queries**: With XPath, you can create precise queries to extract specific data. For example, you can target elements with specific attributes, such as selecting all `<a>` elements with a particular class or locating elements within specific parent elements.\n",
    "\n",
    "4. **Text Extraction**: XPath's `text()` function simplifies the extraction of text content from elements. This is particularly useful for scraping text data, such as headlines, paragraphs, or product descriptions.\n",
    "\n",
    "### How to Use XPath:\n",
    "\n",
    "To utilize XPath for web scraping, you typically follow these steps:\n",
    "\n",
    "1. **Send an HTTP Request**: Use a library like requests to send an HTTP GET request to the webpage you want to scrape. This retrieves the HTML content of the page.\n",
    "\n",
    "2. **Parse the HTML**: Once you have the HTML content, parse it using a library like lxml or lxml.html. This step constructs a structured representation of the webpage that you can navigate with XPath.\n",
    "\n",
    "3. **Construct XPath Expressions**: Formulate XPath expressions that target the specific elements or data you wish to extract. XPath expressions can vary in complexity, allowing you to adapt to different webpage structures.\n",
    "\n",
    "4. **Apply XPath Expressions**: Apply your XPath expressions to the parsed HTML document to select the desired elements or data. This process effectively filters the HTML content to capture only what you need.\n",
    "\n",
    "5. **Retrieve and Process Data**: Retrieve the selected elements or data using the XPath queries and process them as needed for your scraping task.\n",
    "\n",
    "In summary, XPath is a powerful tool for web scraping that offers precise and efficient element selection within HTML documents. While libraries like Beautiful Soup and requests are valuable, XPath provides an additional layer of control and flexibility, making it a valuable choice for advanced scraping projects.\n",
    "\n",
    "\n",
    "### Understanding XPath Syntax\n",
    "\n",
    "- **Absolute Path (`/`)**: \n",
    "  - Using a single slash indicates an absolute path from the root element.\n",
    "  - Example: `xpath('/html/body/p')` selects all paragraph (`<p>`) elements directly under the `<body>` within the `<html>` root element.\n",
    "\n",
    "- **Relative Path (`//`)**:\n",
    "  - Double slashes indicate a relative path, meaning the selection can start anywhere in the document hierarchy.\n",
    "  - Example: `xpath('//a/div')` finds all `<div>` elements that are descendants of `<a>` tags, regardless of their specific location in the document.\n",
    "\n",
    "- **Wildcards (`*`)**:\n",
    "  - The asterisk acts as a wildcard, representing any element.\n",
    "  - Example: `xpath('//a/div/*')` selects all elements that are children of `<div>` tags under `<a>` tags, anywhere in the document.\n",
    "  - Another example: `xpath('/*/*/div')` finds `<div>` elements that are at the second level of the hierarchy from the root.\n",
    "\n",
    "- **Selecting Specific Elements (Using Brackets)**:\n",
    "  - If a selection returns multiple elements, you can specify which one to select using brackets.\n",
    "  - Example: `xpath('//a/div[1]')` selects the first `<div>` in the set; `xpath('//a/div[last()]')` selects the last `<div>`.\n",
    "\n",
    "### Working with Attributes\n",
    "\n",
    "- **Selecting Attributes (`@`)**:\n",
    "  - The `@` symbol is used to work with element attributes.\n",
    "  - Example: `xpath('//@name')` selects all attributes named 'name' in the document.\n",
    "  - To select `<div>` elements with a 'name' attribute: `xpath('//div[@name]')`.\n",
    "  - To select `<div>` elements without any attributes: `xpath('//div[not(@*)]')`.\n",
    "  - To find `<div>` elements with a specific 'name' attribute value: `xpath('//div[@name=\"chachiname\"]')`.\n",
    "\n",
    "### Utilizing Built-in Functions\n",
    "\n",
    "- XPath comes with several built-in functions to aid in element selection.\n",
    "  - `contains()`: Selects elements containing a specific substring. Example: `xpath('//*[contains(name(),'iv')]')`.\n",
    "  - `count()`: Used for conditional selection based on child count. Example: `xpath('//*[count(div)=2]')`.\n",
    "\n",
    "### Combining Paths and Selecting Relatives\n",
    "\n",
    "- **Combining Paths (`|`)**:\n",
    "  - Use the pipe symbol to combine paths, functioning like an OR operator.\n",
    "  - Example: `xpath('/div/p|/div/a')` selects elements matching either `div/p` or `div/a`.\n",
    "\n",
    "- **Selecting Relatives**:\n",
    "  - You can refer to various relational aspects like parent, ancestors, children, or descendants.\n",
    "  - Example: `xpath('//div/div/parent::*')` selects the parent elements of `div/div` paths.\n",
    "\n",
    "Understanding XPath is essential for effective web scraping, as it allows precise targeting and extraction of data based on the structure of a webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import html\n",
    "import requests\n",
    "\n",
    "# URL of the website we want to scrape\n",
    "# For this example, we'll use a news website like the BBC technology page\n",
    "url = \"https://www.bbc.co.uk/news/technology\"\n",
    "\n",
    "# Send a GET request to the URL to fetch the webpage's HTML content\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content of the webpage\n",
    "# The html.fromstring method constructs an lxml HTML document from the response text\n",
    "tree = html.fromstring(response.content)\n",
    "\n",
    "# Use XPath to select specific elements\n",
    "# In this case, we'll attempt to extract text content from the page\n",
    "# The XPath expression here captures all text content within the HTML structure\n",
    "# Note: The actual XPath may vary depending on the webpage's HTML structure\n",
    "headlines = tree.xpath('//text()')\n",
    "\n",
    "\n",
    "# Print each extracted headline\n",
    "# The text() function in XPath extracts the text content of the selected elements\n",
    "for headline in headlines:\n",
    "    print(headline.strip())\n",
    "\n",
    "# This script prints all the text content of <h3> tags found on the BBC Technology page\n",
    "# It demonstrates how to use XPath for extracting specific information from a webpage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0. Starting with Selenium \n",
    "\n",
    "Selenium is a powerful tool primarily used for automating web browsers. It's widely utilized in areas such as web scraping, automated testing, and automating web-based administration tasks.\n",
    "\n",
    "### Introduction to Selenium Without Geckodriver\n",
    "\n",
    "Traditionally, Selenium works in conjunction with a driver specific to each browser, like geckodriver for Firefox or chromedriver for Chrome. However, recent developments have enabled certain browsers to be controlled directly by Selenium without the need for an additional driver:\n",
    "\n",
    "- **Chrome**: Recent versions of Google Chrome can be controlled by Selenium directly through the Chrome DevTools Protocol. This simplifies the setup process as you don't need to download and set up chromedriver separately.\n",
    "\n",
    "- **Microsoft Edge**: Similar to Chrome, the Edge browser (Chromium version) can also be automated directly using Selenium with its built-in driver capabilities. \n",
    "\n",
    "This approach of using Selenium without an additional driver streamlines browser automation tasks, making it more accessible and easier to configure, especially for beginners and those looking to quickly set up automated browser interactions.\n",
    "\n",
    "## 2.1 Basic Concepts of Selenium WebDriver\n",
    "\n",
    "### Understanding WebDriver\n",
    "\n",
    "WebDriver is a key component of the Selenium suite. It acts as an interface to interact with the web browser, allowing you to control it programmatically. WebDriver can perform operations like opening web pages, clicking buttons, entering text in forms, and extracting data from web pages.\n",
    "\n",
    "#### Key Functions of WebDriver\n",
    "- **Opening a Web Page**: WebDriver can navigate to a specific URL.\n",
    "- **Locating Elements**: It can find elements on a web page based on their attributes (like ID, name, XPath).\n",
    "- **Interacting with Elements**: WebDriver can simulate actions like clicking buttons, typing text, and submitting forms.\n",
    "\n",
    "### Interacting with Web Elements\n",
    "\n",
    "You can locate and interact with elements on a web page using various methods provided by WebDriver. The choice of method depends on the attributes of the HTML elements you're targeting.\n",
    "\n",
    "- **find_element_by_id**: Locates an element by its unique ID.\n",
    "- **find_element_by_name**: Finds an element by its name attribute.\n",
    "- **find_element_by_xpath**: Uses XPath queries to locate elements, providing a powerful way to navigate the DOM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Selenium WebDriver Python Examples\n",
    "#### Example 1: Opening a Web Page\n",
    "\n",
    "#This example demonstrates how to open a web page using Selenium WebDriver.\n",
    "\n",
    "from selenium import webdriver\n",
    "\n",
    "# Initialize the Chrome WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Open a web page\n",
    "driver.get(\"https://www.python.org\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the browser\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 2: Web Scraping NBA Player Salaries**\n",
    "\n",
    "This example demonstrates how to scrape NBA player salary data from a website using Selenium in Python. It's a practical illustration of how Selenium can be utilized to automate web browsing and extract specific data from web pages. The script navigates through different pages for each NBA season, collects player names and their corresponding salaries, and organizes this data into a pandas DataFrame for each year from 1990 to 2018. This is a useful example for learning how to manage web elements, extract text, and handle data using pandas in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary libraries\n",
    "from selenium import webdriver  # Used to automate web browser interaction\n",
    "from selenium.webdriver.common.by import By  # Helps in locating elements on web pages\n",
    "import pandas as pd  # Pandas library for data manipulation and analysis\n",
    "\n",
    "# Creating an empty DataFrame with specified columns\n",
    "# This DataFrame will be used to store the scraped data\n",
    "df = pd.DataFrame(columns=['Player', 'Salary', 'Year'])\n",
    "\n",
    "# Initializing the Chrome WebDriver\n",
    "# This opens up a Chrome browser window for web scraping\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Looping through the years 2017 to 2018\n",
    "for yr in range(2017, 2019):\n",
    "    # Constructing the URL for each year by appending the year range to the base URL\n",
    "    page_num = str(yr) + '-' + str(yr + 1) + '/'\n",
    "    url = 'https://hoopshype.com/salaries/players/' + page_num\n",
    "    driver.get(url)  # Navigating to the constructed URL in the browser\n",
    "    \n",
    "    # Finding all player name elements on the page using their XPATH\n",
    "    # XPATH is a syntax used to navigate through elements and attributes in an XML document\n",
    "    players = driver.find_elements(By.XPATH, '//td[@class=\"name\"]')\n",
    "\n",
    "    # Similarly, finding all salary elements on the page using their XPATH\n",
    "    salaries = driver.find_elements(By.XPATH, '//td[@class=\"hh-salaries-sorted\"]')\n",
    "    \n",
    "    # Extracting the text from each player element and storing in a list\n",
    "    players_list = [player.text for player in players]\n",
    "\n",
    "    # Extracting the text from each salary element and storing in a list\n",
    "    salaries_list = [salary.text for salary in salaries]\n",
    "    \n",
    "    # Pairing each player's name with their salary and year using the zip function\n",
    "    data_tuples = list(zip(players_list[1:], salaries_list[1:]))\n",
    "    \n",
    "    # Creating a temporary DataFrame for the current year\n",
    "    # This DataFrame contains the player names, their salaries, and the year\n",
    "    temp_df = pd.DataFrame(data_tuples, columns=['Player', 'Salary'])\n",
    "    temp_df['Year'] = yr\n",
    "\n",
    "    # Appending the temporary DataFrame to the master DataFrame\n",
    "    # ignore_index=True is used to ensure the index continues correctly in the master DataFrame\n",
    "    df = df.append(temp_df, ignore_index=True)\n",
    "\n",
    "# Closing the WebDriver after completing the scraping\n",
    "# This is important to free up resources and avoid potential memory leaks\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business challenge: **Analyzing Barcelona's Rental Market: A Web Scraping and Data Visualization Project**\n",
    "\n",
    "## Objective:\n",
    "The goal is to develop a Python-based web scraper to extract rental property data from Idealista for different neighborhoods in Barcelona. This data will be analyzed using Power BI to uncover insights into the city's rental market.\n",
    "\n",
    "## Scope:\n",
    "- **Web Scraping**: Extract key data points such as rental prices, property size, number of bedrooms, and neighborhood locations from Idealista.\n",
    "- **Data Analysis and Visualization**: Analyze the scraped data to identify trends and patterns, then visualize these findings using Power BI.\n",
    "\n",
    "## Steps and Agile Methodology Application:\n",
    "\n",
    "### 1. Project Initiation and Planning (Sprint 0)\n",
    "- **Team Setup**: Form cross-functional teams with roles such as Scrum Master, Product Owner and Data Analysts.\n",
    "- **Requirement Gathering**: Define the specific data points to be scraped from Idealista.\n",
    "- **Tool Selection**: Choose appropriate tools for web scraping (e.g., Python with libraries like BeautifulSoup, Selenium) and for data visualization (Power BI).\n",
    "- **Backlog Creation**: Create a product backlog comprising user stories (e.g., \"As a data analyst, I want to scrape rental prices so that I can analyze the average rent in each neighborhood\").\n",
    "\n",
    "### 2. Sprint Execution\n",
    "- **Sprint Planning**: Break down the backlog into smaller, manageable tasks to be completed in each sprint (e.g., setting up the scraping environment, designing the data model, etc.).\n",
    "- **Daily Stand-ups**: Hold brief daily meetings to discuss progress, roadblocks, and next steps.\n",
    "- **Development and Testing**: Perform iterative development, with regular testing to ensure data accuracy and reliability.\n",
    "- **Sprint Review**: At the end of each sprint, review the work completed and demonstrate the functionality.\n",
    "- **Sprint Retrospective**: Reflect on the sprint process to identify improvements for the next sprint.\n",
    "\n",
    "### 3. Web Scraping Phase\n",
    "- Implement web scraping scripts to extract the required data from Idealista.\n",
    "- Ensure compliance with Idealista's web scraping policies and legal considerations.\n",
    "\n",
    "### 4. Data Analysis and Visualization\n",
    "- Clean and preprocess the scraped data for analysis.\n",
    "- Use Power BI to create interactive dashboards and visualizations that highlight key aspects of the rental market in Barcelona.\n",
    "\n",
    "### 5. Final Review and Presentation\n",
    "- Compile the findings and insights into a comprehensive report.\n",
    "- Present the data analysis and visualizations to stakeholders or in a class setting.\n",
    "\n",
    "### Deliverables:\n",
    "- Source code for the web scraping script.\n",
    "- Power BI dashboard files.\n",
    "- Final report detailing methodology, findings, and insights.\n",
    "\n",
    "### Learning Outcomes:\n",
    "- Practical application of web scraping and data analysis.\n",
    "- Experience in using Agile methodology for project management.\n",
    "- Enhanced collaboration and teamwork skills.\n",
    "- Proficiency in using Python for data collection and Power BI for data visualization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the BBC technology news section\n",
    "url = \"https://www.bbc.co.uk/news/technology\"\n",
    "\n",
    "# Send a GET request and parse the HTML content\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Define categories and associated keywords\n",
    "categories = {\n",
    "    'Apple': ['Apple', 'iPhone', 'iPad'],\n",
    "    'Microsoft': ['Microsoft', 'Windows', 'Bill Gates'],\n",
    "    'Google': ['Google', 'Android', 'Alphabet']\n",
    "    # Add more categories as needed\n",
    "}\n",
    "\n",
    "# Function to determine the category of a headline\n",
    "def categorize_headline(headline):\n",
    "    for category, keywords in categories.items():\n",
    "        for keyword in keywords:\n",
    "            if keyword in headline:\n",
    "                return category\n",
    "    return 'Other'\n",
    "\n",
    "# Scrape and process the headlines\n",
    "# Look for 'h3' tags or other relevant tags\n",
    "headlines = soup.find_all('h3')\n",
    "category_counts = {category: 0 for category in categories.keys()}\n",
    "category_counts['Other'] = 0\n",
    "\n",
    "for h in headlines:\n",
    "    headline_text = h.get_text().strip()\n",
    "    category = categorize_headline(headline_text)\n",
    "    category_counts[category] += 1\n",
    "    print(f\"Headline: {headline_text}\\nCategory: {category}\\n\")\n",
    "\n",
    "# Print the count of headlines in each category\n",
    "print(\"Headline Counts by Category:\")\n",
    "for category, count in category_counts.items():\n",
    "    print(f\"{category}: {count}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
